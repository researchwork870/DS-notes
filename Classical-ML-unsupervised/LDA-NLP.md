# Latent Dirichlet Allocation (LDA)

## 1. Intuition and Mathematical Foundations

### Intuition
Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora. The core idea behind LDA is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.

LDA is based on the following intuitive assumptions:
- Each document contains a mixture of topics
- Each topic is a distribution over the vocabulary of words
- Words in a document are generated by first selecting a topic from the document's topic distribution, then selecting a word from that topic's word distribution

For example, a document about sports might be 70% about basketball, 20% about general fitness, and 10% about nutrition. The "basketball" topic might have high probabilities for words like "court," "dribble," and "NBA," while the "nutrition" topic might emphasize words like "protein," "vitamins," and "diet."

### Mathematical Foundations

LDA is a Bayesian hierarchical model with the following formal definition:

1. **Corpus-level parameters**:
   - α: Dirichlet parameter for document-topic distribution (typically a symmetric prior)
   - β: Dirichlet parameter for topic-word distribution (typically a symmetric prior)
   - K: Number of topics

2. **Document-level variables**:
   - θd: Topic distribution for document d (drawn from Dirichlet(α))

3. **Word-level variables**:
   - zd,n: Topic assignment for nth word in document d
   - wd,n: The observed nth word in document d

The generative process for a document corpus is as follows:

1. For each topic k = 1, ..., K:
   - Draw a word distribution φk ~ Dirichlet(β)

2. For each document d = 1, ..., M:
   - Draw a topic distribution θd ~ Dirichlet(α)
   - For each word position n = 1, ..., Nd in document d:
     - Draw a topic assignment zd,n ~ Multinomial(θd)
     - Draw a word wd,n ~ Multinomial(φzd,n)

The joint probability distribution factorizes as:

$$p(\mathbf{w}, \mathbf{z}, \mathbf{\theta}, \mathbf{\phi} | \alpha, \beta) = \prod_{k=1}^K p(\phi_k | \beta) \prod_{d=1}^M p(\theta_d | \alpha) \prod_{n=1}^{N_d} p(z_{d,n} | \theta_d) p(w_{d,n} | \phi_{z_{d,n}})$$

### Algorithm Implementation (Theoretical)

The goal of LDA is to infer the posterior distribution:

$$p(\mathbf{z}, \mathbf{\theta}, \mathbf{\phi} | \mathbf{w}, \alpha, \beta)$$

Since this posterior is intractable to compute exactly, several inference algorithms have been developed:

#### Collapsed Gibbs Sampling

1. **Initialization**: Randomly assign each word in every document to one of the K topics.

2. **Counting**: 
   - n(d,k): Number of words in document d assigned to topic k
   - n(k,w): Number of times word w is assigned to topic k
   - n(k): Total number of words assigned to topic k

3. **Sampling**: For each word w in each document d:
   - Remove the current topic assignment of this word instance
   - Update counts: Decrement n(d,k), n(k,w), and n(k)
   - Sample a new topic k for this word according to:
   
   $$p(z_{i} = k | \mathbf{z}_{-i}, \mathbf{w}) \propto \frac{n(d,k)_{-i} + \alpha_k}{n(d)_{-i} + \sum_j \alpha_j} \cdot \frac{n(k,w)_{-i} + \beta_w}{n(k)_{-i} + \sum_v \beta_v}$$
   
   - Update counts: Increment n(d,k), n(k,w), and n(k) for the new topic assignment

4. **Iteration**: Repeat sampling step for a large number of iterations or until convergence.

5. **Parameter Estimation**: After convergence, estimate the topic-word and document-topic distributions:
   
   $$\phi_{k,w} = \frac{n(k,w) + \beta_w}{n(k) + \sum_v \beta_v}$$
   
   $$\theta_{d,k} = \frac{n(d,k) + \alpha_k}{n(d) + \sum_j \alpha_j}$$

#### Variational Inference

An alternative inference approach uses variational methods:

1. **Define a variational distribution** with simplified dependency structure:
   
   $$q(\mathbf{z}, \mathbf{\theta}, \mathbf{\phi}) = \prod_{k=1}^K q(\phi_k | \lambda_k) \prod_{d=1}^M q(\theta_d | \gamma_d) \prod_{n=1}^{N_d} q(z_{d,n} | \phi_{d,n})$$
   
   where:
   - q(φk | λk) is Dirichlet(λk)
   - q(θd | γd) is Dirichlet(γd)
   - q(zd,n | φd,n) is Multinomial(φd,n)

2. **Optimize variational parameters** (λ, γ, φ) to minimize the Kullback-Leibler divergence between the variational distribution and the true posterior.

3. **Update Equations**:
   
   - Update φd,n,k (probability of topic k for word n in document d):
     
     $$\phi_{d,n,k} \propto \exp\{E_q[\log \theta_{d,k}] + E_q[\log \phi_{k,w_{d,n}}]\}$$
   
   - Update γd,k (parameter for document-topic distribution):
     
     $$\gamma_{d,k} = \alpha_k + \sum_{n=1}^{N_d} \phi_{d,n,k}$$
   
   - Update λk,w (parameter for topic-word distribution):
     
     $$\lambda_{k,w} = \beta_w + \sum_{d=1}^M \sum_{n=1}^{N_d} \phi_{d,n,k} \cdot \mathbf{1}[w_{d,n} = w]$$

4. **Iterate** until convergence of the ELBO (Evidence Lower BOund).

## 2. Key Hyperparameters

### Number of Topics (K)
- **Definition**: The number of latent topics to be discovered in the corpus.
- **Effect**: 
  - Too few topics: Results in overly broad, general topics that are hard to distinguish.
  - Too many topics: Creates overlapping or redundant topics and may fit to noise.
- **Selection**: Often determined through model comparison metrics like perplexity, coherence scores, or via domain expertise.

### Document-Topic Prior (α)
- **Definition**: Dirichlet prior parameter for document-topic distribution.
- **Range**: Typically α > 0, often set to be symmetric (same value for all topics).
- **Effect**:
  - Lower values (α < 1): Sparse topic distributions, documents tend to contain fewer topics.
  - Higher values (α > 1): Smooth topic distributions, documents tend to contain many topics.
  - α = 1: Uniform prior (no preference for sparsity or smoothness).
- **Common settings**: 50/K or 5/K (where K is the number of topics).

### Topic-Word Prior (β)
- **Definition**: Dirichlet prior parameter for topic-word distribution.
- **Range**: Typically β > 0, often set as symmetric (same value for all words).
- **Effect**:
  - Lower values (β < 1): Sparse word distributions, topics contain fewer significant words.
  - Higher values (β > 1): Smooth word distributions, topics contain many words.
  - β = 1: Uniform prior.
- **Common settings**: 0.01 to 0.1.

### Number of Iterations
- **Definition**: Number of sampling iterations or variational inference steps.
- **Effect**: 
  - Too few: Inference may not converge.
  - Too many: Computational inefficiency without significant improvement.
- **Typical range**: 500-2000 iterations, with burn-in period for Gibbs sampling.

### Burn-in Period (for Gibbs Sampling)
- **Definition**: Initial sampling iterations that are discarded before collecting samples.
- **Effect**: Allows Markov chain to reach stationary distribution before using samples.
- **Typical range**: 50-200 iterations.

### Thinning Interval (for Gibbs Sampling)
- **Definition**: Taking every nth sample after burn-in to reduce autocorrelation.
- **Effect**: Reduces correlation between consecutive samples but increases computation.
- **Typical range**: 5-20 iterations.

### Convergence Threshold
- **Definition**: Tolerance level to determine algorithm convergence.
- **Effect**: Balances precision and computational efficiency.
- **Typical range**: 10^-4 to 10^-6 change in log-likelihood or parameter values.

## 3. Strengths, Weaknesses, and Use Cases

### Strengths
- **Interpretable Topics**: Produces human-interpretable topics as distributions over words.
- **Unsupervised Learning**: Requires no labeled data.
- **Mixed Membership**: Documents can belong to multiple topics with different proportions.
- **Probabilistic Framework**: Provides uncertainty measures and principled Bayesian approach.
- **Scalability**: Can be implemented efficiently for large text corpora.
- **Flexibility**: Can be extended to incorporate additional information (e.g., author, time).
- **Document Representation**: Creates low-dimensional representations of documents for downstream tasks.

### Weaknesses
- **Topic Number Selection**: Requires pre-specifying the number of topics K.
- **Basic Model Limitations**: 
  - Bag-of-words assumption (ignores word order)
  - Exchangeability assumption (ignores document structure)
  - Fixed vocabulary (struggles with out-of-vocabulary words)
- **Topic Quality**: May produce incoherent or redundant topics.
- **Lack of Temporal Dynamics**: Standard LDA doesn't model how topics evolve over time.
- **Computational Intensity**: Inference can be computationally expensive for large corpora.
- **Sensitivity to Hyperparameters**: Results can vary significantly based on α and β values.
- **Short Text Challenges**: Performs poorly on short documents with limited word co-occurrence.

### Appropriate Use Cases
- **Document Exploration**: Understanding themes in large document collections.
- **Content Recommendation**: Suggesting similar content based on topic similarity.
- **Information Retrieval**: Enhancing search by matching queries to topic distributions.
- **Document Classification**: Using topic distributions as features for classification.
- **Trend Analysis**: Tracking topic evolution when extended with temporal components.
- **Author-Topic Modeling**: Analyzing writing styles and author interests.
- **Scientific Literature Analysis**: Discovering research trends and patterns.
- **Customer Review Analysis**: Extracting product aspects and sentiment patterns.
- **News Categorization**: Automatically organizing news articles by content.

## 4. Common Evaluation Metrics

### Perplexity
- **Definition**: Measures how surprised a model is by new data (lower is better).
- **Formula**: 
  $$\text{Perplexity}(W_{\text{test}}) = \exp\left(-\frac{\sum_{d=1}^M \log p(w_d)}{N}\right)$$
  where N is the total number of words.
- **Interpretation**: Lower perplexity indicates better generalization performance.
- **Limitations**: Does not necessarily correlate with human judgment of topic quality.

### Topic Coherence
- **Definition**: Measures semantic similarity among top words within a topic.
- **Variants**:
  - **C_v**: Based on a combination of normalized pointwise mutual information (NPMI) and cosine similarity.
  - **C_uci**: Based on pointwise mutual information (PMI).
  - **C_npmi**: Based on normalized pointwise mutual information.
  - **U_mass**: Based on document co-occurrence.
- **Interpretation**: Higher coherence values indicate more coherent, interpretable topics.

### Topic Diversity
- **Definition**: Measures how distinct topics are from each other.
- **Calculation methods**:
  - Pairwise Jaccard distance between top words
  - Average Hellinger distance between topic distributions
- **Interpretation**: Higher diversity indicates less redundancy among topics.

### Held-out Likelihood
- **Definition**: Log-likelihood of unseen documents under the trained model.
- **Interpretation**: Higher likelihood indicates better generalization.
- **Implementation**: Often approximated via importance sampling or variational methods.

### Classification Accuracy
- **Definition**: When using topic distributions as features for classification tasks.
- **Interpretation**: Higher accuracy indicates more discriminative topic representations.

### Human Evaluation
- **Intrusion tasks**: 
  - Word intrusion: Humans identify which word doesn't belong in a topic.
  - Topic intrusion: Humans identify which topic doesn't belong to a document.
- **Quality ratings**: Human experts rate topics for coherence and usefulness.
- **Interpretation**: Higher human agreement with model expectations indicates better topic quality.

### Topic Stability
- **Definition**: Consistency of discovered topics across multiple runs.
- **Measurement**: 
  - Average Jaccard similarity between top words of matched topics
  - Topic alignment metrics
- **Interpretation**: Higher stability indicates more robust topic discovery.

## 5. Advanced Variants and Extensions

### Correlated Topic Model (CTM)
- Replaces Dirichlet with logistic normal distribution to model correlation between topics
- Captures relationships between topics that LDA cannot

### Hierarchical Dirichlet Process (HDP)
- Non-parametric extension that automatically determines the number of topics
- Uses a hierarchical structure of Dirichlet processes

### Dynamic Topic Model (DTM)
- Extends LDA to incorporate time, allowing topics to evolve
- Uses state space models to capture temporal dependencies

### Supervised LDA (sLDA)
- Incorporates document labels to guide topic discovery
- Jointly models documents and their associated response variables

### Author-Topic Model
- Associates authors with topics, not just documents
- Models author-specific topic distributions

### Sentence-LDA and Sequential-LDA
- Respects document structure by modeling sentences or sequences
- Relaxes bag-of-words assumption within units

### Online LDA
- Processes documents incrementally, suitable for streaming data
- Uses stochastic variational inference for efficiency

### Sparse Topic Models
- Enforces sparsity constraints on topic and word distributions
- Often produces more interpretable topics

### Topic-Word Priors (TWP)
- Incorporates prior knowledge about word-topic associations
- Guides topic discovery with domain expertise

### GPU-accelerated and Distributed LDA
- Implementations optimized for parallel processing
- Enables analysis of extremely large corpora